# SkillsBench Overview

  Core Purpose: SkillsBench is a benchmark framework that evaluates how effectively AI agents leverage
   "skills" (structured packages of procedural knowledge) to complete tasks. It measures skill
  efficacy through paired evaluation.

  Key Methodology:
  - 84 tasks across 11 domains with deterministic verifiers
  - Three evaluation conditions per task:
    - No Skills: Baseline agent performance
    - With Curated Skills: Agent has access to expert-authored skills
    - Self-Generated Skills: Agent attempts to create its own skills
  - Measures the delta between conditions to isolate skill impact

  Major Findings:
  1. Curated skills improve performance by +16.2pp on average (but with high variance)
  2. Self-generated skills provide negligible/negative benefit (-1.3pp)
  3. 2-3 focused skills outperform comprehensive documentation
  4. Skills are most effective in domains underrepresented in model pretraining

  Relevance to Cogworks

  Strong Alignment: Testing Skills Generated by Cogworks

  SkillsBench directly addresses Layer 2.5+ evaluation - does the skill actually improve agent
  performance on real tasks? This complements cogworks' existing testing layers:

  Current cogworks testing (from TESTING.md):
  - Layer 1: Structural validation (deterministic-checks.sh)
  - Layer 2: Semantic quality (LLM-as-judge via /cogworks-test)
  - Layer 2.5: Behavioral activation (tests/behavioral/)
  - Layer 3: Calibration gate

  SkillsBench provides:
  - Efficacy measurement: Paired evaluation (with/without skill) quantifies actual performance
  improvement
  - Validation of approach: The finding that self-generated skills fail validates cogworks' strategy
  of curating from authoritative sources
  - Design principles: Insights on optimal skill structure (2-3 modules, focused vs. comprehensive)

  Key Insights for Cogworks

  1. Self-generated skills don't work: SkillsBench shows models can't reliably author effective
  procedural knowledge (-1.3pp vs. +16.2pp for curated). This strongly validates cogworks' approach of
   synthesizing from authoritative sources rather than pure LLM generation.
  2. Quality > quantity: 2-3 skills optimal (+18.6pp), while 4+ skills show diminishing returns
  (+5.9pp). Comprehensive skills hurt performance (-2.9pp). This suggests cogworks should favor
  focused, modular outputs.
  3. Procedural gaps matter most: Skills show largest improvements in domains with specialized
  workflows underrepresented in pretraining (Healthcare +51.9pp, Manufacturing +41.9pp vs. Software
  Engineering +4.5pp).

  Limitations for Direct Integration

  SkillsBench is a benchmark, not a testing framework:
  - Requires curated task creation (84 tasks from 322 submissions, 26.7% acceptance rate)
  - Resource-intensive: 7,308 trajectories, containerized environments, deterministic verifiers
  - Focus on terminal-based tasks may not generalize to all cogworks use cases

  Separation of concerns (per TESTING.md):
  - SkillsBench tests: "Does skill X improve performance on task Y?"
  - Cogworks pipeline testing: "Does the cogworks pipeline correctly synthesize skills from sources?"
  - These are related but distinct validation needs

  Practical Applications

  Could adopt from SkillsBench:

  1. Paired evaluation methodology: Extend behavioral tests to include "without skill" baseline
  measurements
    - Current: Tests if skill activates correctly
    - Enhanced: Tests if skill improves task completion vs. baseline
  2. Efficacy metrics: Add pass rate delta and normalized gain to behavioral test reports
  3. Benchmark integration: Use SkillsBench tasks as a validation dataset for cogworks-generated
  skills
    - Generate skills from SkillsBench task documentation
    - Compare cogworks output against SkillsBench curated skills
    - Measure relative performance
  4. Design validation: Use SkillsBench findings to validate cogworks skill structure decisions
    - Module count (2-3 optimal)
    - Documentation length (focused > comprehensive)
    - Supporting files strategy

  Should NOT adopt:
  - Full SkillsBench infrastructure (too heavyweight for cogworks pipeline validation)
  - Task creation burden (cogworks needs lightweight, automated testing)
  - Three-condition evaluation for every test (only needed for efficacy validation, not pipeline
  testing)

  Recommendation

  SkillsBench is highly relevant for testing skills generated by cogworks (the second concern in
  TESTING.md), but less directly applicable to testing the cogworks pipeline itself.

  Concrete next steps:
  1. Maintain separation: Keep using current framework for cogworks pipeline testing (deterministic
  checks, semantic quality, behavioral activation)
  2. Add paired evaluation: Enhance behavioral tests to include baseline comparisons:
  # Current: Does skill activate?
  python3 .claude/test-framework/scripts/cogworks-test-framework.py behavioral run --skill
  cogworks-encode

  # Enhanced: Does skill improve performance vs. baseline?
  python3 .claude/test-framework/scripts/cogworks-test-framework.py efficacy run --skill
  my-generated-skill
  3. Benchmark validation (optional): For promoting skills to "golden samples", run them through a
  subset of SkillsBench tasks to validate efficacy against established baseline
  4. Design principles: Use SkillsBench findings (2-3 modules, focused guidance, procedural emphasis)
  to inform cogworks generation strategy

  The key insight: SkillsBench validates that the type of skills cogworks generates (curated from
  authoritative sources) significantly outperform self-generated alternatives, and provides empirical
  guidance on optimal skill structure.
